{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trazas para diferentes tipos de ejecuciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tipos de Runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangSmith admite muchos tipos diferentes de Runs. Podés especificar el tipo de Run en el decorador @traceable.\n",
    "Los tipos de runs son:\n",
    "\n",
    "- LLM: Invoca un modelo de lenguaje.\n",
    "- Retriever: Recupera documentos desde bases de datos u otras fuentes.\n",
    "- Tool: Ejecuta acciones mediante llamadas a funciones.\n",
    "- Chain: Tipo por defecto; combina múltiples runs en un proceso más grande.\n",
    "- Prompt: Inyecta un prompt para ser usado con un LLM.\n",
    "- Parser: Extrae datos estructurados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\".env\", override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invocaciones a un LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangSmith ofrece un renderizado y procesamiento especial para las trazas de LLM. Para aprovechar al máximo esta funcionalidad, tenés que registrar (loggear) tus trazas de LLM en un formato específico."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para modelos de estilo chat, las entradas deben ser una lista de mensajes en formato compatible con OpenAI, representados como diccionarios de Python u objetos de TypeScript. Cada mensaje debe incluir las claves role y content.\n",
    "\n",
    "La salida puede estar en cualquiera de estos formatos:\n",
    "- Un diccionario/objeto que tenga la clave choices, cuyo valor sea una lista de diccionarios/objetos. Cada uno debe contener la clave message, que corresponde a un objeto mensaje con role y content.\n",
    "- Un diccionario/objeto que tenga la clave message, cuyo valor sea un objeto mensaje con role y content.\n",
    "- Una tupla/arreglo de dos elementos, donde el primero es el role y el segundo es el content.\n",
    "- Un diccionario/objeto que contenga directamente las claves role y content.\n",
    "La entrada de tu función debe llamarse messages.\n",
    "\n",
    "También podés proveer los siguientes metadatos para ayudar a LangSmith a identificar el modelo y calcular costos. Si usás LangChain o el wrapper de OpenAI, estos campos se completan automáticamente:\n",
    "- ls_provider: el proveedor del modelo, por ejemplo \"openai\" o \"anthropic\".\n",
    "- ls_model_name: el nombre del modelo, por ejemplo \"gpt-4o-mini\" o \"claude-3-opus-20240307\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'choices': [{'message': {'role': 'assistant',\n",
       "    'content': 'Claro, ¿para qué hora le gustaría reservar la mesa?'}}]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langsmith import traceable\n",
    "\n",
    "inputs = [\n",
    "  {\"role\": \"system\", \"content\": \"Sos un asistente servicial.\"},\n",
    "  {\"role\": \"user\", \"content\": \"Me gustaría reservar una mesa para dos.\"},\n",
    "]\n",
    "\n",
    "output = {\n",
    "  \"choices\": [\n",
    "      {\n",
    "          \"message\": {\n",
    "              \"role\": \"assistant\",\n",
    "              \"content\": \"Claro, ¿para qué hora le gustaría reservar la mesa?\"\n",
    "          }\n",
    "      }\n",
    "  ]\n",
    "}\n",
    "\n",
    "\n",
    "# También se puede usar uno de los siguientes:\n",
    "# output = {\n",
    "#     \"message\": {\n",
    "#         \"role\": \"assistant\",\n",
    "#         \"content\": \"Claro, ¿para qué hora le gustaría reservar la mesa?\"\n",
    "#     }\n",
    "# }\n",
    "#\n",
    "# output = {\n",
    "#     \"role\": \"assistant\",\n",
    "#     \"content\": \"Claro, ¿para qué hora le gustaría reservar la mesa?\"\n",
    "# }\n",
    "#\n",
    "# output = [\"assistant\", \"Claro, ¿para qué hora le gustaría reservar la mesa?\"]\n",
    "\n",
    "@traceable(\n",
    "  #  run_type=\"llm\"\n",
    ")\n",
    "def chat_model(messages: list):\n",
    "  return output\n",
    "\n",
    "chat_model(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recuperación de documentos de bases de datos u otras fuentes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muchas aplicaciones de LLM necesitan buscar documentos en bases vectoriales, grafos de conocimiento u otros tipos de índices. Las retriever traces sirven para registrar los documentos que recupera el retriever. LangSmith ofrece un renderizado especial para estos pasos de recuperación dentro de las trazas, lo que facilita entender y diagnosticar problemas de búsqueda. Para que los pasos de recuperación se muestren correctamente, hay que seguir algunos pasos simples:\n",
    "\n",
    "1. Anotar el paso del retriever con run_type=\"retriever\".\n",
    "2. Devolver una lista de diccionarios de Python u objetos de TypeScript desde el paso del retriever.\n",
    "Cada diccionario debe incluir:\n",
    "    - page_content: el texto del documento.\n",
    "    - type: siempre debe ser \"Document\".\n",
    "    - metadata: un diccionario u objeto con metadatos sobre el documento, que se mostrará en la traza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_content': 'LangSmith Document contents 1',\n",
       "  'type': 'Document',\n",
       "  'metadata': {'foo': 'bar'}},\n",
       " {'page_content': 'LangSmith Document contents 2',\n",
       "  'type': 'Document',\n",
       "  'metadata': {'foo': 'bar'}},\n",
       " {'page_content': 'LangSmith Document contents 3',\n",
       "  'type': 'Document',\n",
       "  'metadata': {'foo': 'bar'}}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langsmith import traceable\n",
    "\n",
    "def _convert_docs(results):\n",
    "  return [\n",
    "      {\n",
    "          \"page_content\": r,\n",
    "          \"doc_type\": \"Document\"\n",
    "          #\"type\": \"Document\",\n",
    "          \"metadata\": {\"foo\": \"bar\"}\n",
    "      }\n",
    "      for r in results\n",
    "  ]\n",
    "\n",
    "@traceable(\n",
    "    #run_type=\"retriever\"\n",
    ")\n",
    "def retrieve_langsmith_docs(query):\n",
    "  # Retriever que devuelve documentos de prueba hardcodeados.\n",
    "  # En producción, esto podría ser una base de datos vectorial real u otro tipo de índice de documentos.\n",
    "  contents = [\"LangSmith Document contents 1\", \"LangSmith Document contents 2\", \"LangSmith Document contents 3\"]\n",
    "  return _convert_docs(contents)\n",
    "\n",
    "retrieve_langsmith_docs(\"User query\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llamada a herramientas (tools) y Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangSmith tiene un renderizado especial para las Tool Calls realizadas por el modelo, para dejar claro cuándo se están utilizando las herramientas proporcionadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-CdfooBhyveGCJETfbY1LFoaGJrHMP', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Lo siento, no puedo acceder a información en tiempo real sobre el clima. Te recomiendo que consultes una aplicación de clima o un sitio web confiable para obtener la información más actualizada sobre el clima en Buenos Aires.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1763571578, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint='fp_560af6e559', usage=CompletionUsage(completion_tokens=44, prompt_tokens=92, total_tokens=136, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langsmith import traceable\n",
    "from openai import OpenAI\n",
    "from typing import List, Optional\n",
    "import json\n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n",
    "@traceable(\n",
    "    run_type=\"tool\"\n",
    ")\n",
    "def search_web(query: str):\n",
    "    return f\"No se encontró ningún resultado para la consulta: {query}\"\n",
    "\n",
    "@traceable(run_type=\"llm\")\n",
    "def call_openai(\n",
    "    messages: List[dict], tools: Optional[List[dict]]\n",
    ") -> str:\n",
    "  return openai_client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=messages,\n",
    "    temperature=0,\n",
    "    tools=tools\n",
    "  )\n",
    "\n",
    "@traceable(run_type=\"chain\")\n",
    "def respond(inputs, tools):\n",
    "  response = call_openai(inputs, tools)\n",
    "  tool_call_args = json.loads(response.choices[0].message.tool_calls[0].function.arguments)\n",
    "  query = tool_call_args[\"query\"]\n",
    "  tool_response_message = {\n",
    "    \"role\": \"tool\",\n",
    "    \"content\": json.dumps({\n",
    "        \"query\": query,\n",
    "        \"results\": search_web(query),\n",
    "    }),\n",
    "    \"tool_call_id\": response.choices[0].message.tool_calls[0].id\n",
    "  }\n",
    "  inputs.append(response.choices[0].message)\n",
    "  inputs.append(tool_response_message)\n",
    "  output = call_openai(inputs, None)\n",
    "  return output\n",
    "\n",
    "tools = [\n",
    "    {\n",
    "      \"type\": \"function\",\n",
    "      \"function\": {\n",
    "        \"name\": \"search_web\",\n",
    "        \"description\": \"Buscar en la web una consulta específica\",\n",
    "        \"parameters\": {\n",
    "          \"type\": \"object\",\n",
    "          \"properties\": {\n",
    "            \"query\": {\n",
    "              \"type\": \"string\",\n",
    "              \"description\": \"La consulta que se va a buscar en la web\"\n",
    "            }\n",
    "          },\n",
    "          \"required\": [\"query\"]\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "]\n",
    "inputs = [\n",
    "  {\"role\": \"system\", \"content\": \"Sos un asistente servicial.\"},\n",
    "  {\"role\": \"user\", \"content\": \"Cómo está el clima hoy en Buenos Aires?\"},\n",
    "]\n",
    "\n",
    "respond(inputs, tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
