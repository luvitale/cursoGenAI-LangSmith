{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explicamelo como si tuviera 5 años\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Architecture](images/architecture.png)\n",
    "\n",
    "En este cuaderno vamos a recorrer el proceso de configurar un chatbot simple en LangChain.\n",
    "\n",
    "A lo largo de este proceso, vamos a mostrar cómo LangSmith puede usarse para mejorar la experiencia del desarrollador en aplicaciones de IA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empecemos cargando nuestras variables de entorno desde el archivo .env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\".env\", override=True)\n",
    "# Cargar las siguientes variables de entorno:\n",
    "# LANGSMITH_TRACING=true\n",
    "# LANGSMITH_ENDPOINT=\"https://api.smith.langchain.com\"\n",
    "# LANGSMITH_PROJECT=\"eli5-bot\"\n",
    "# LANGSMITH_API_KEY=\"<redacted>\"\n",
    "\n",
    "# OPENAI_API_KEY=\"<redacted>\"\n",
    "# TAVILY_API_KEY=\"<redacted>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuremos una herramienta llamada Tavily para permitir que nuestro asistente busque en la web al responder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "web_search_tool = TavilySearchResults(max_results=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diseñemos un prompt para RAG que usaremos a lo largo de todo el cuaderno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt Template:  Sos un profesor y un experto en explicar temas complejos de una manera fácil de entender.\n",
      "Tu trabajo es responder la pregunta dada de forma tal que incluso un niño de 5 años pueda comprenderla.\n",
      "Se te ha proporcionado el contexto necesario para responder la pregunta.\n",
      "\n",
      "Pregunta: {question} \n",
      "\n",
      "Contexto: {context}\n",
      "\n",
      "Respuesta:\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Sos un profesor y un experto en explicar temas complejos de una manera fácil de entender.\n",
    "Tu trabajo es responder la pregunta dada de forma tal que incluso un niño de 5 años pueda comprenderla.\n",
    "Se te ha proporcionado el contexto necesario para responder la pregunta.\n",
    "\n",
    "Pregunta: {question} \n",
    "\n",
    "Contexto: {context}\n",
    "\n",
    "Respuesta:\"\"\"\n",
    "print(\"Prompt Template: \", prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creando nuestra aplicación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from langsmith import traceable\n",
    "from langsmith.wrappers import wrap_openai\n",
    "\n",
    "openai_client = wrap_openai(OpenAI())\n",
    "\n",
    "@traceable\n",
    "def search(question):\n",
    "    web_docs = web_search_tool.invoke({\"query\": question})\n",
    "    web_results = \"\\n\".join([d[\"content\"] for d in web_docs])\n",
    "    return web_results\n",
    "    \n",
    "@traceable\n",
    "def explain(question, context):\n",
    "    formatted = prompt.format(question=question, context=context)\n",
    "    \n",
    "    completion = openai_client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": formatted},\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "        ],\n",
    "        model=\"gpt-4o-mini\",\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "@traceable\n",
    "def eli5(question):\n",
    "    context = search(question)\n",
    "    answer = explain(question, context)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testeando nuestra application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Claro! Imagina que la economía es como una gran fiesta con muchas personas. La macroeconomía es la parte que mira a toda la fiesta en lugar de solo a una persona o a un grupo pequeño.\n",
      "\n",
      "Entonces, en vez de fijarnos en lo que le pasa a un niño con su juguete, la macroeconomía observa cosas como cuántos niños están en la fiesta, si hay suficiente comida para todos, y si están todos felices o no. Eso incluye cosas como si la gente tiene trabajo, si los precios de las cosas suben o bajan, y cómo los adultos (los gobiernos) tratan de que todo funcione mejor.\n",
      "\n",
      "Así que, en resumen, la macroeconomía nos ayuda a entender cómo va la fiesta grande que es la economía de un país.\n"
     ]
    }
   ],
   "source": [
    "question = \"Qué es la macroeconomía?\"\n",
    "print(eli5(question))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
