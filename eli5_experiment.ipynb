{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar variables de entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(dotenv_path=\".env\", override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crear aplicación AI "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como siempre, definamos nuestro prompt y demos a nuestra aplicación acceso a la web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sergi\\AppData\\Local\\Temp\\ipykernel_55712\\750042844.py:4: LangChainDeprecationWarning: The class `TavilySearchResults` was deprecated in LangChain 0.3.25 and will be removed in 1.0. An updated version of the class exists in the `langchain-tavily package and should be used instead. To use it run `pip install -U `langchain-tavily` and import as `from `langchain_tavily import TavilySearch``.\n",
      "  web_search_tool = TavilySearchResults(max_results=1)\n"
     ]
    }
   ],
   "source": [
    "# Inicializar herramienta de búsqueda web.\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "web_search_tool = TavilySearchResults(max_results=1)\n",
    "\n",
    "# Definir prompt template\n",
    "prompt = \"\"\"Sos un profesor y un experto en explicar temas complejos de una manera fácil de entender.\n",
    "Tu trabajo es responder la pregunta dada de forma que incluso un niño de 5 años pueda comprenderla.\n",
    "Se te ha brindado el contexto necesario para responder la pregunta.\n",
    "\n",
    "Pregunta: {question} \n",
    "\n",
    "Contexto: {context}\n",
    "\n",
    "Respuesta:\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definir la lógica de la aplicación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La lógica acá es la misma que en el módulo de trazas. Definimos un paso de búsqueda para explorar la web y un paso de explicación para que un modelo de lenguaje resuma los resultados encontrados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from langsmith import traceable\n",
    "from langsmith.wrappers import wrap_openai\n",
    "\n",
    "\n",
    "# Crear application\n",
    "openai_client = wrap_openai(OpenAI())\n",
    "\n",
    "@traceable\n",
    "def search(question):\n",
    "    web_docs = web_search_tool.invoke({\"query\": question})\n",
    "    web_results = \"\\n\".join([d[\"content\"] for d in web_docs])\n",
    "    return web_results\n",
    "    \n",
    "@traceable\n",
    "def explain(question, context):\n",
    "    formatted = prompt.format(question=question, context=context)\n",
    "    \n",
    "    completion = openai_client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": formatted},\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "        ],\n",
    "        model=\"o3-mini\",\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "@traceable\n",
    "def eli5(question):\n",
    "    context = search(question)\n",
    "    answer = explain(question, context)\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup del experimento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora estamos listos para ejecutar experimentos y probar el rendimiento de nuestra aplicación sobre nuestro dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importar cliente LangSmith "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero, vamos a crear un cliente de LangSmith para usar el SDK y especificar el dataset sobre el que queremos ejecutar nuestro experimento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "#dataset_name = \"eli5-silver\"\n",
    "dataset_name = \"ds-new-crystallography-60\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definir evaluadores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluador de código personalizado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero definiremos un evaluador de código personalizado, que resulta útil para medir métricas deterministas o de respuesta cerrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conciseness(outputs: dict) -> bool:\n",
    "    words = outputs[\"output\"].split(\" \")\n",
    "    return len(words) <= 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este evaluador de código personalizado es simplemente una función de Python que verifica si nuestra aplicación produce respuestas de 200 palabras o menos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LLM-as-a-Judge Evaluador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para métricas abiertas, puede ser muy potente usar un LLM para puntuar las respuestas.\n",
    "\n",
    "Usemos un LLM para comprobar si nuestra aplicación produce resultados correctos. Primero, definamos un esquema de puntuación que nuestro LLM deba seguir en su respuesta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Definir un esquema de puntuación al que nuestro LLM debe ajustarse.\n",
    "class CorrectnessScore(BaseModel):\n",
    "    \"\"\"Correctness score of the answer when compared to the reference answer.\"\"\"\n",
    "    score: int = Field(description=\"The score of the correctness of the answer, from 0 to 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a definir una función para darle a un LLM las salidas de nuestra aplicación, junto con las salidas de referencia guardadas en nuestro conjunto de datos.\n",
    "\n",
    "De este modo, el LLM podrá usar la respuesta “correcta” como referencia para juzgar si la respuesta de nuestra aplicación cumple con nuestros estándares de precisión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "\n",
    "def correctness(inputs: dict, outputs: dict, reference_outputs: dict) -> bool:\n",
    "    prompt = \"\"\"\n",
    "    Sos un etiquetador de datos experto que evalúa las respuestas de un modelo para verificar su corrección.\n",
    "Tu tarea es asignar una puntuación basada en la siguiente rúbrica:\n",
    "\n",
    "    <Rubric>\n",
    "        Una respuesta correcta:\n",
    "            - Brinda información precisa\n",
    "            - Usa analogías y ejemplos adecuados\n",
    "            - No contiene errores fácticos\n",
    "            - Es lógicamente consistente\n",
    "\n",
    "        Al puntuar, debés penalizar:\n",
    "            - Errores fácticos\n",
    "            - Analogías y ejemplos incoherentes\n",
    "            - Inconsistencias lógicas    \n",
    "    </Rubric>\n",
    "\n",
    "    <Instructions>\n",
    "        - Leé con atención la entrada y la salida.\n",
    "        - Usá la salida de referencia para determinar si la salida del modelo contiene errores.\n",
    "        - Concentrate en si la salida del modelo usa analogías precisas y es lógicamente consistente.\n",
    "    </Instructions>\n",
    "\n",
    "    <Reminder>\n",
    "        Las analogías de la salida no necesitan coincidir exactamente con la salida de referencia. Concentrate en la consistencia lógica.\n",
    "    </Reminder>\n",
    "\n",
    "    <input>\n",
    "        {}\n",
    "    </input>\n",
    "\n",
    "    <output>\n",
    "        {}\n",
    "    </output>\n",
    "\n",
    "    Usá las salidas de referencia que aparecen abajo para ayudarte a evaluar la corrección de la respuesta.\n",
    "    <reference_outputs>\n",
    "        {}\n",
    "    </reference_outputs>\n",
    "    \"\"\".format(inputs[\"question\"], outputs[\"output\"], reference_outputs[\"output\"])\n",
    "    structured_llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0).with_structured_output(CorrectnessScore)\n",
    "    generation = structured_llm.invoke([HumanMessage(content=prompt)])\n",
    "    return generation.score == 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definir Run Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a definir una función para ejecutar nuestra aplicación sobre las entradas de ejemplo de nuestro dataset. Esta es la función que se va a llamar cuando ejecutemos nuestro experimento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir una función para ejecutar tu aplicación.\n",
    "def run(inputs: dict):\n",
    "    return eli5(inputs[\"question\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correr experimento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos todos los componentes necesarios, así que ejecutemos nuestro experimento!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sergi\\Documents\\langsmith\\eli5\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'eli5-o4-mini-e36162a4' at:\n",
      "https://smith.langchain.com/o/11afda74-8804-4cb8-8ad4-c2a9d67d44e5/datasets/0a5a0302-cfaf-44b4-9de9-e4b312e514fd/compare?selectedSessions=3af3ae00-128f-46f1-9e99-55d2e97544d5\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]c:\\Users\\sergi\\Documents\\langsmith\\eli5\\.venv\\Lib\\site-packages\\pydantic\\v1\\main.py:1054: UserWarning: LangSmith now uses UUID v7 for run and trace identifiers. This warning appears when passing custom IDs. Please use: from langsmith import uuid7\n",
      "            id = uuid7()\n",
      "Future versions will require UUID v7.\n",
      "  input_data = validator(cls_, input_data)\n",
      "10it [01:20,  8.04s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs.question</th>\n",
       "      <th>outputs.output</th>\n",
       "      <th>error</th>\n",
       "      <th>reference.output</th>\n",
       "      <th>feedback.correctness</th>\n",
       "      <th>feedback.conciseness</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>example_id</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Qué es la macroeconomía?</td>\n",
       "      <td>Imagina que la economía de un país es como una...</td>\n",
       "      <td>None</td>\n",
       "      <td>La macroeconomía es como una lupa gigante que ...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>5.993207</td>\n",
       "      <td>15d24932-295f-473e-b1d2-086f1bd2f32d</td>\n",
       "      <td>019a9efa-135a-7140-a800-9171ae7f4598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why is the sky blue?</td>\n",
       "      <td>Imagine the Sun’s light is like a box of crayo...</td>\n",
       "      <td>None</td>\n",
       "      <td>Alright! Imagine the sky is like a big bowl of...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>4.682507</td>\n",
       "      <td>1af9dae1-8130-46b1-9efd-2b9a381f2b0f</td>\n",
       "      <td>019a9efa-3393-71a5-80f1-eec579ee8f8e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How does string theory work?</td>\n",
       "      <td>Imagina que todo lo que ves, incluso tú y yo, ...</td>\n",
       "      <td>None</td>\n",
       "      <td>Okay! Imagine that everything in the universe,...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>8.736447</td>\n",
       "      <td>4054da81-5869-484d-916c-e12c19059078</td>\n",
       "      <td>019a9efa-484d-70d1-9b1a-10772ba94e9c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How does photosynthesis work?</td>\n",
       "      <td>Imagina que una planta es como una cocinita má...</td>\n",
       "      <td>None</td>\n",
       "      <td>Okay! Imagine plants are like tiny chefs, and ...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>7.778148</td>\n",
       "      <td>8edadc77-fb59-4aa1-9bb3-4a0e369c50af</td>\n",
       "      <td>019a9efa-6dfd-7269-aebf-4e27b9b4b710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is trustcall library?</td>\n",
       "      <td>Imagina que tienes un gran libro de dibujos y ...</td>\n",
       "      <td>None</td>\n",
       "      <td>Alright, imagine you have a toy box where each...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>6.479814</td>\n",
       "      <td>a661d0ce-9672-4a25-b813-104ea367db23</td>\n",
       "      <td>019a9efa-9048-7794-88fa-5b4c99d46d7d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>How does a democracy work?</td>\n",
       "      <td>Imagina que tú y tus amigos quieren escoger un...</td>\n",
       "      <td>None</td>\n",
       "      <td>Okay! Imagine you and your friends want to dec...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>7.127248</td>\n",
       "      <td>acd456da-cd68-4842-bcfe-60049549e0eb</td>\n",
       "      <td>019a9efa-aca1-740e-b4d6-ebd6e79a809c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What is LangSmith by LangChain?</td>\n",
       "      <td>Imagine you built a little talking robot that ...</td>\n",
       "      <td>None</td>\n",
       "      <td>Okay! Imagine you have a big box of toys that ...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>10.696698</td>\n",
       "      <td>b80f2c35-2d23-400d-9963-83cfdb4a0026</td>\n",
       "      <td>019a9efa-ccc5-745d-86ec-0d3c6981801b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What is the Langchain framework?</td>\n",
       "      <td>Imagina que quieres montar un castillo de LEGO...</td>\n",
       "      <td>None</td>\n",
       "      <td>Okay! Imagine you want to build a really cool ...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>7.235491</td>\n",
       "      <td>bb1147ea-5e7a-4445-bba4-fcafb900d5dc</td>\n",
       "      <td>019a9efa-f96d-7300-9cd3-4c710edd6e79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What is sound?</td>\n",
       "      <td>Sound is what we hear when something “wiggles”...</td>\n",
       "      <td>None</td>\n",
       "      <td>Okay! Imagine you have a drum. When you hit it...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>5.162881</td>\n",
       "      <td>bf115918-1d03-48c0-a4ef-a33ee567533f</td>\n",
       "      <td>019a9efb-1b8d-748c-9841-131e0a90c24a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What is LangGraph?</td>\n",
       "      <td>Imagina que quieres construir un castillo de L...</td>\n",
       "      <td>None</td>\n",
       "      <td>Okay, imagine you have a big box of LEGO brick...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>5.745662</td>\n",
       "      <td>d7a5d945-7662-45ab-9099-634e1544c56c</td>\n",
       "      <td>019a9efb-327b-775a-ae02-928dcea607e8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<ExperimentResults eli5-o4-mini-e36162a4>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langsmith import evaluate\n",
    "\n",
    "evaluate(\n",
    "    run,\n",
    "    data=dataset_name,\n",
    "    evaluators=[correctness, conciseness],\n",
    "    experiment_prefix=\"eli5-o3-mini\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
