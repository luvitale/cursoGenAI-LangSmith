{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar variables de entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(dotenv_path=\".env\", override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crear aplicación AI "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como siempre, definamos nuestro prompt y demos a nuestra aplicación acceso a la web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sergi\\AppData\\Local\\Temp\\ipykernel_57576\\750042844.py:4: LangChainDeprecationWarning: The class `TavilySearchResults` was deprecated in LangChain 0.3.25 and will be removed in 1.0. An updated version of the class exists in the `langchain-tavily package and should be used instead. To use it run `pip install -U `langchain-tavily` and import as `from `langchain_tavily import TavilySearch``.\n",
      "  web_search_tool = TavilySearchResults(max_results=1)\n"
     ]
    }
   ],
   "source": [
    "# Inicializar herramienta de búsqueda web.\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "web_search_tool = TavilySearchResults(max_results=1)\n",
    "\n",
    "# Definir prompt template\n",
    "prompt = \"\"\"Sos un profesor y un experto en explicar temas complejos de una manera fácil de entender.\n",
    "Tu trabajo es responder la pregunta dada de forma que incluso un niño de 5 años pueda comprenderla.\n",
    "Se te ha brindado el contexto necesario para responder la pregunta.\n",
    "\n",
    "Pregunta: {question} \n",
    "\n",
    "Contexto: {context}\n",
    "\n",
    "Respuesta:\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definir la lógica de la aplicación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La lógica acá es la misma que en el módulo de trazas. Definimos un paso de búsqueda para explorar la web y un paso de explicación para que un modelo de lenguaje resuma los resultados encontrados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from langsmith import traceable\n",
    "from langsmith.wrappers import wrap_openai\n",
    "\n",
    "\n",
    "# Crear application\n",
    "openai_client = wrap_openai(OpenAI())\n",
    "\n",
    "@traceable\n",
    "def search(question):\n",
    "    web_docs = web_search_tool.invoke({\"query\": question})\n",
    "    web_results = \"\\n\".join([d[\"content\"] for d in web_docs])\n",
    "    return web_results\n",
    "    \n",
    "@traceable\n",
    "def explain(question, context):\n",
    "    formatted = prompt.format(question=question, context=context)\n",
    "    \n",
    "    completion = openai_client.chat.completions.create(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": formatted},\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "        ],\n",
    "        model=\"o3-mini\",\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "@traceable\n",
    "def eli5(question):\n",
    "    context = search(question)\n",
    "    answer = explain(question, context)\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup del experimento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora estamos listos para ejecutar experimentos y probar el rendimiento de nuestra aplicación sobre nuestro dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importar cliente LangSmith "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero, vamos a crear un cliente de LangSmith para usar el SDK y especificar el dataset sobre el que queremos ejecutar nuestro experimento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "#dataset_name = \"eli5-silver\"\n",
    "dataset_name = \"ds-new-crystallography-60\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definir evaluadores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluador de código personalizado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero definiremos un evaluador de código personalizado, que resulta útil para medir métricas deterministas o de respuesta cerrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conciseness(outputs: dict) -> bool:\n",
    "    words = outputs[\"output\"].split(\" \")\n",
    "    return len(words) <= 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este evaluador de código personalizado es simplemente una función de Python que verifica si nuestra aplicación produce respuestas de 200 palabras o menos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LLM-as-a-Judge Evaluador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para métricas abiertas, puede ser muy potente usar un LLM para puntuar las respuestas.\n",
    "\n",
    "Usemos un LLM para comprobar si nuestra aplicación produce resultados correctos. Primero, definamos un esquema de puntuación que nuestro LLM deba seguir en su respuesta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Definir un esquema de puntuación al que nuestro LLM debe ajustarse.\n",
    "class CorrectnessScore(BaseModel):\n",
    "    \"\"\"Correctness score of the answer when compared to the reference answer.\"\"\"\n",
    "    score: int = Field(description=\"The score of the correctness of the answer, from 0 to 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define a function to give an LLM our application's outputs, alongside the reference outputs stored in our dataset. \n",
    "\n",
    "The LLM will then be able to reference the \"right\" output to judge if our application's answer meets our accuracy standards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "\n",
    "def correctness(inputs: dict, outputs: dict, reference_outputs: dict) -> bool:\n",
    "    prompt = \"\"\"\n",
    "    You are an expert data labeler evaluating model outputs for correctness. Your task is to assign a score based on the following rubric:\n",
    "\n",
    "    <Rubric>\n",
    "        A correct answer:\n",
    "        - Provides accurate information\n",
    "        - Uses suitable analogies and examples\n",
    "        - Contains no factual errors\n",
    "        - Is logically consistent\n",
    "\n",
    "        When scoring, you should penalize:\n",
    "        - Factual errors\n",
    "        - Incoherent analogies and examples\n",
    "        - Logical inconsistencies\n",
    "    </Rubric>\n",
    "\n",
    "    <Instructions>\n",
    "        - Carefully read the input and output\n",
    "        - Use the reference output to determine if the model output contains errors\n",
    "        - Focus whether the model output uses accurate analogies and is logically consistent\n",
    "    </Instructions>\n",
    "\n",
    "    <Reminder>\n",
    "        The analogies in the output do not need to match the reference output exactly. Focus on logical consistency.\n",
    "    </Reminder>\n",
    "\n",
    "    <input>\n",
    "        {}\n",
    "    </input>\n",
    "\n",
    "    <output>\n",
    "        {}\n",
    "    </output>\n",
    "\n",
    "    Use the reference outputs below to help you evaluate the correctness of the response:\n",
    "    <reference_outputs>\n",
    "        {}\n",
    "    </reference_outputs>\n",
    "    \"\"\".format(inputs[\"question\"], outputs[\"output\"], reference_outputs[\"output\"])\n",
    "    structured_llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0).with_structured_output(CorrectnessScore)\n",
    "    generation = structured_llm.invoke([HumanMessage(content=prompt)])\n",
    "    return generation.score == 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Run Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll define a function to run our application on the example inputs of our dataset. This is function that will be called when we run our experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Define a function to run your application\n",
    "def run(inputs: dict):\n",
    "    return eli5(inputs[\"question\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have all the necessary components, so let's run our experiment! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sergi\\Documents\\langsmith\\eli5\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for experiment: 'eli5-o3-mini-61a07e79' at:\n",
      "https://smith.langchain.com/o/11afda74-8804-4cb8-8ad4-c2a9d67d44e5/datasets/0a5a0302-cfaf-44b4-9de9-e4b312e514fd/compare?selectedSessions=8d191364-db83-488c-a737-6a5f8a44bd6f\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]c:\\Users\\sergi\\Documents\\langsmith\\eli5\\.venv\\Lib\\site-packages\\pydantic\\v1\\main.py:1054: UserWarning: LangSmith now uses UUID v7 for run and trace identifiers. This warning appears when passing custom IDs. Please use: from langsmith import uuid7\n",
      "            id = uuid7()\n",
      "Future versions will require UUID v7.\n",
      "  input_data = validator(cls_, input_data)\n",
      "10it [01:49, 10.94s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs.question</th>\n",
       "      <th>outputs.output</th>\n",
       "      <th>error</th>\n",
       "      <th>reference.output</th>\n",
       "      <th>feedback.correctness</th>\n",
       "      <th>feedback.conciseness</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>example_id</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Qué es la macroeconomía?</td>\n",
       "      <td>Imagínate que la economía es como un enorme ro...</td>\n",
       "      <td>None</td>\n",
       "      <td>La macroeconomía es como una lupa gigante que ...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>7.366929</td>\n",
       "      <td>15d24932-295f-473e-b1d2-086f1bd2f32d</td>\n",
       "      <td>019a9ed9-85d0-763c-bb46-778f015b7991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why is the sky blue?</td>\n",
       "      <td>Imagine you have a box with lots of colored pe...</td>\n",
       "      <td>None</td>\n",
       "      <td>Alright! Imagine the sky is like a big bowl of...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>10.035590</td>\n",
       "      <td>1af9dae1-8130-46b1-9efd-2b9a381f2b0f</td>\n",
       "      <td>019a9ed9-b613-721b-860f-856d0931ffdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How does string theory work?</td>\n",
       "      <td>Imagine that everything in the universe is mad...</td>\n",
       "      <td>None</td>\n",
       "      <td>Okay! Imagine that everything in the universe,...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>11.949423</td>\n",
       "      <td>4054da81-5869-484d-916c-e12c19059078</td>\n",
       "      <td>019a9ed9-e157-74c9-be41-7562da2ffc48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How does photosynthesis work?</td>\n",
       "      <td>Imagine that a plant is like a little chef in ...</td>\n",
       "      <td>None</td>\n",
       "      <td>Okay! Imagine plants are like tiny chefs, and ...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>11.735456</td>\n",
       "      <td>8edadc77-fb59-4aa1-9bb3-4a0e369c50af</td>\n",
       "      <td>019a9eda-1376-732b-ab95-2d8f0b5d46c9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is trustcall library?</td>\n",
       "      <td>Imagine you have a big box of LEGOs that you u...</td>\n",
       "      <td>None</td>\n",
       "      <td>Alright, imagine you have a toy box where each...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>9.853126</td>\n",
       "      <td>a661d0ce-9672-4a25-b813-104ea367db23</td>\n",
       "      <td>019a9eda-47d5-71a7-b6fb-b4d515be7d42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>How does a democracy work?</td>\n",
       "      <td>Imagine you and your friends need to choose wh...</td>\n",
       "      <td>None</td>\n",
       "      <td>Okay! Imagine you and your friends want to dec...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>6.848612</td>\n",
       "      <td>acd456da-cd68-4842-bcfe-60049549e0eb</td>\n",
       "      <td>019a9eda-72f8-7778-b5f3-1c1ac59e2739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What is LangSmith by LangChain?</td>\n",
       "      <td>Imagine you have a very smart toy robot that t...</td>\n",
       "      <td>None</td>\n",
       "      <td>Okay! Imagine you have a big box of toys that ...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>8.322277</td>\n",
       "      <td>b80f2c35-2d23-400d-9963-83cfdb4a0026</td>\n",
       "      <td>019a9eda-909a-7055-86b0-7ac733be8468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What is the Langchain framework?</td>\n",
       "      <td>Imagine you have a big box of colorful LEGO br...</td>\n",
       "      <td>None</td>\n",
       "      <td>Okay! Imagine you want to build a really cool ...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>9.396697</td>\n",
       "      <td>bb1147ea-5e7a-4445-bba4-fcafb900d5dc</td>\n",
       "      <td>019a9eda-b503-757f-bd9d-71c345e378c3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What is sound?</td>\n",
       "      <td>Imagine you’re playing with a drum. When you h...</td>\n",
       "      <td>None</td>\n",
       "      <td>Okay! Imagine you have a drum. When you hit it...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>8.593642</td>\n",
       "      <td>bf115918-1d03-48c0-a4ef-a33ee567533f</td>\n",
       "      <td>019a9eda-ddae-74fe-8fc4-b369cd7ada2d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What is LangGraph?</td>\n",
       "      <td>Imagine you have a big box of LEGO pieces, and...</td>\n",
       "      <td>None</td>\n",
       "      <td>Okay, imagine you have a big box of LEGO brick...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>10.463656</td>\n",
       "      <td>d7a5d945-7662-45ab-9099-634e1544c56c</td>\n",
       "      <td>019a9edb-0246-776d-a1c9-a8ce5ca8d223</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<ExperimentResults eli5-o3-mini-61a07e79>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langsmith import evaluate\n",
    "\n",
    "evaluate(\n",
    "    run,\n",
    "    data=dataset_name,\n",
    "    evaluators=[correctness, conciseness],\n",
    "    experiment_prefix=\"eli5-o3-mini\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
