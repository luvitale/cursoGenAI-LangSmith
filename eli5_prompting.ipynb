{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Engineering en LangSmith"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importamos variables de entorno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\".env\", override=True)\n",
    "LANGSMITH_API_KEY = os.getenv(\"LANGSMITH_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejecutamos un Pull del Prompt desde Prompthub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "client = Client(api_key=LANGSMITH_API_KEY)\n",
    "prompt = client.pull_prompt(\"eli5-profesor\", include_model=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Aplicación IA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero configuremos nuestra herramienta de búsqueda web, como siempre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sergi\\AppData\\Local\\Temp\\ipykernel_55428\\632376592.py:3: LangChainDeprecationWarning: The class `TavilySearchResults` was deprecated in LangChain 0.3.25 and will be removed in 1.0. An updated version of the class exists in the `langchain-tavily package and should be used instead. To use it run `pip install -U `langchain-tavily` and import as `from `langchain_tavily import TavilySearch``.\n",
      "  web_search_tool = TavilySearchResults(max_results=1)\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "web_search_tool = TavilySearchResults(max_results=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora creemos nuestra aplicación, igual que en el módulo de trazas. Esta vez, nuestro prompt es el que se obtuvo desde PromptHub.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from langsmith import traceable\n",
    "from langsmith.wrappers import wrap_openai\n",
    "\n",
    "# Creamos la aplicación\n",
    "openai_client = wrap_openai(OpenAI())\n",
    "\n",
    "@traceable\n",
    "def search(question):\n",
    "    web_docs = web_search_tool.invoke({\"query\": question})\n",
    "    web_results = \"\\n\".join([d[\"content\"] for d in web_docs])\n",
    "    return web_results\n",
    "    \n",
    "@traceable\n",
    "def explain(question, context):\n",
    "    prompt_inputs = {\"question\": question, \"context\": context}\n",
    "    messages = prompt.invoke(prompt_inputs)\n",
    "    messages = [{\"role\": messages.type, \"content\": messages.content}]\n",
    "    # Mapea los roles a los valores válidos de OpenAI\n",
    "    role_map = {\"ai\": \"assistant\", \"human\": \"user\"}\n",
    "    for msg in messages:\n",
    "        if msg[\"role\"] in role_map:\n",
    "            msg[\"role\"] = role_map[msg[\"role\"]]\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "@traceable\n",
    "def eli5(question):\n",
    "    context = search(question)\n",
    "    answer = explain(question, context)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sergi\\Documents\\langsmith\\eli5\\.venv\\Lib\\site-packages\\pydantic\\v1\\main.py:1054: UserWarning: LangSmith now uses UUID v7 for run and trace identifiers. This warning appears when passing custom IDs. Please use: from langsmith import uuid7\n",
      "            id = uuid7()\n",
      "Future versions will require UUID v7.\n",
      "  input_data = validator(cls_, input_data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La macroeconomía es la rama de la economía que analiza y estudia el comportamiento de la economía en su conjunto, en lugar de enfocarse en mercados individuales o sectores específicos. Se interesa en variables agregadas como el producto interno bruto (PIB), la tasa de desempleo, la inflación y el crecimiento económico. \n",
      "\n",
      "Al igual que cuidar de un jardín grande, donde cada planta representa un sector de la economía, la macroeconomía busca entender cómo interactúan todos estos elementos para asegurar un crecimiento saludable y sostenible. Los líderes y responsables de la política económica utilizan la macroeconomía para tomar decisiones que pueden afectar a todos los ciudadanos, buscando mejorar el bienestar general. Por ejemplo, pueden implementar políticas fiscales o monetarias para estimular la economía o controlar la inflación, con el objetivo de crear un entorno en el que todos puedan prosperar.\n"
     ]
    }
   ],
   "source": [
    "question = \"Qué es la macroeconomía?\"\n",
    "print(eli5(question))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
